{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static input\n",
    "env = {}\n",
    "env['static_input'] = {'start_date':'2000-11-21', 'end_date':'2000-11-21', 'horizontal':'004','vertical':'002'}\n",
    "env['predecessor_outputs'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USERNAME'] = ''\n",
    "os.environ['PASSWORD'] = ''\n",
    "os.environ['REGION'] = 'CU'\n",
    "os.environ['GEOJSON'] = 'geojson/meadow.geojson'\n",
    "os.environ['DATA_SET_NAME'] = 'ARD_TILE'\n",
    "os.environ['PRODUCT'] = 'SR'\n",
    "os.environ['FILEPATH'] = './tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phenology: \n",
    "    def get_apikey(username, password):\n",
    "        payload = dict(username=username, password=password, catalogId='EE', authType='EROS')\n",
    "        data = dict(jsonRequest=json.dumps(payload))\n",
    "        r = requests.post('https://earthexplorer.usgs.gov/inventory/json/v/1.4.0/login', data=data)\n",
    "        creds = r.json()\n",
    "        return creds['data']\n",
    "    \n",
    "    def get_scenes(apiKey, region, horizontal, vertical, start_date, end_date):\n",
    "        myfilter = {\n",
    "                \"filterType\": \"and\", \n",
    "                \"childFilters\": [\n",
    "                    {\"filterType\":\"value\",\"fieldId\":21789,\"value\":region}, #Grid Region\n",
    "                    {\"filterType\":\"value\",\"fieldId\":21787,\"value\":horizontal}, #Horizontal\n",
    "                    {\"filterType\":\"value\",\"fieldId\":21788,\"value\":vertical}, #Vertical\n",
    "                ]\n",
    "            }\n",
    "        startDate, endDate = Phenology.get_dates(start_date, end_date)\n",
    "        temporalfilter = {\n",
    "            \"startDate\": startDate,\n",
    "            \"endDate\": endDate\n",
    "        }\n",
    "        request_code = 'search'\n",
    "        baseurl = f'https://earthexplorer.usgs.gov/inventory/json/v/1.4.0/{request_code}'\n",
    "        payload = {'apiKey':apiKey, \n",
    "                   'datasetName': os.environ['DATA_SET_NAME'], \n",
    "                   \"temporalFilter\": temporalfilter,\n",
    "                   'additionalCriteria': myfilter,  \n",
    "                   'maxResults':10000}\n",
    "        data = dict(jsonRequest=json.dumps(payload))\n",
    "        r = requests.get(baseurl, params=data)\n",
    "        response = r.json()\n",
    "        return response\n",
    "    \n",
    "    # 15 day buffer before start date and after end date\n",
    "    def get_dates(start_date, end_date):\n",
    "        start_date = datetime(int(start_date[:4]), int(start_date[5:7]), int(start_date[-2:]))\n",
    "        end_date = datetime(int(end_date[:4]), int(end_date[5:7]), int(end_date[-2:]))\n",
    "        start = (start_date - timedelta(days=15)).strftime('%Y-%m-%d')\n",
    "        end = (end_date + timedelta(days=15)).strftime('%Y-%m-%d')\n",
    "        return start, end\n",
    "    \n",
    "    def get_url(entityId, apiKey, product=os.environ['PRODUCT']):\n",
    "        request_code = 'download'\n",
    "        baseurl = f'https://earthexplorer.usgs.gov/inventory/json/v/1.4.0/{request_code}'\n",
    "        payload = {'apiKey':apiKey, \n",
    "                   'datasetName': os.environ['DATA_SET_NAME'], \n",
    "                   'entityIds':[entityId],\n",
    "                   'products':[product]} #these are 'download codes' from above\n",
    "        data = dict(jsonRequest=json.dumps(payload))\n",
    "        r = requests.get(baseurl, params=data)\n",
    "        data = r.json()['data']\n",
    "        download_url = r.json()['data'][0]['url']\n",
    "        p = urlparse(download_url)\n",
    "        name = os.path.basename(p.path)\n",
    "        return os.environ['FILEPATH'] + name, download_url\n",
    "    \n",
    "    def download(filename, url):\n",
    "        chunks = 1024*1024\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=chunks):\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    def calculate_NDVI(nir_data, red_data, longitudes, latitudes):\n",
    "        np.seterr(invalid='ignore')\n",
    "        x, y = np.meshgrid(longitudes, latitudes)\n",
    "        nir_data = nir_data.astype(float)\n",
    "        nir_data = nir_data.astype(float)\n",
    "        data = np.array([nir_data.flatten(), red_data.flatten(), x.flatten(), y.flatten()])\n",
    "        data = np.rot90(data)\n",
    "        # filter out negative values and values greater than 10,000\n",
    "        data = data[np.logical_and(np.logical_and(data[:, 0] >= 0, data[:, 1] >= 0), np.logical_and(data[:, 0] <= 10000, data[:, 1] <= 10000))]\n",
    "        ndvi = np.array([np.divide(np.subtract(data[:, 0], data[:, 1]), np.add(data[:, 0], data[:, 1]))])\n",
    "        ndvi = np.rot90(ndvi)\n",
    "        data = np.append(data, ndvi, 1)\n",
    "        # filter out invalid ndvi values\n",
    "        data = data[np.logical_and(data[:, 4] >= -1, data[:, 4] <= 1)]\n",
    "        # calculation of values\n",
    "        maximum = data[:, 4].max() if len(data) != 0 else 'invalid'\n",
    "        flowering = data[np.logical_and(data[:, 4] <= .425, data[:, 4] >= .375)]\n",
    "        FF = round(len(flowering)/len(data), 2) if len(data) != 0 else 'invalid'\n",
    "        VF = 1 - FF if len(data) != 0 else 'invalid'\n",
    "        return {\n",
    "            'avg': round(data[:, 4].mean(), 2) if len(data) != 0 else 'invalid',\n",
    "            'max': round(maximum, 2) if len(data) != 0 else 'invalid',\n",
    "            'min': round(data[:, 4].min(), 2) if len(data) != 0 else 'invalid',\n",
    "            'std': round(data[:, 4].std(), 2) if len(data) != 0 else 'invalid',\n",
    "            'FF': FF,\n",
    "            'VF': VF,\n",
    "            'coordinates': json.dumps(data[data[:, 4] == maximum][:, 2:4].round(4).tolist()) if len(data) != 0 else 'invalid'\n",
    "        }\n",
    "        \n",
    "    def get_reflectance(data):\n",
    "        data = data.astype(float)\n",
    "        # filters values between 0 and 10000\n",
    "        data = np.divide(data[np.logical_and(data >= 0,data <= 10000)], 10000)\n",
    "        return {'avg': round(data.mean(), 2) if data.size != 0 else 'invalid', \n",
    "                'max': round(data.max(), 2) if data.size != 0 else 'invalid', \n",
    "                'min': round(data.min(), 2) if data.size != 0 else 'invalid', \n",
    "                'std': round(data.std(), 2) if data.size != 0 else 'invalid'\n",
    "               }\n",
    "    \n",
    "    def get_statistical_values(tar):\n",
    "        values = {}\n",
    "        with tarfile.open(tar) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.name[-8:] in ['SRB5.tif','SRB4.tif', 'SRB3.tif', 'SRB2.tif']:\n",
    "                    path = os.environ['FILEPATH'] + member.name\n",
    "                    tar.extract(member, path=os.environ['FILEPATH'])\n",
    "                    outpath = Phenology.reproject(path, path + 'cropped')\n",
    "                    data = Phenology.crop_raster_data(outpath)\n",
    "                    if member.name[-8:] == 'SRB4.tif':   \n",
    "                        values['red_data'] = data\n",
    "                        with rio.open(outpath) as src:\n",
    "                            transform = src.meta['transform']\n",
    "                            values['longitudes'] = xy(transform, [x for x in range(src.width)], [y for y in range(src.width)])[0]\n",
    "                            values['latitudes'] = xy(transform, [x for x in range(src.height)], [y for y in range(src.height)])[1]\n",
    "                    if member.name[-8:] == 'SRB3.tif':\n",
    "                        values['green_data'] = data\n",
    "                    if member.name[-8:] == 'SRB2.tif':\n",
    "                        values['blue_data'] = data\n",
    "                    if member.name[-8:] == 'SRB5.tif':\n",
    "                        values['nir_data'] = data\n",
    "        return {'blue_ref': Phenology.get_reflectance(values['blue_data']), \n",
    "                'green_ref': Phenology.get_reflectance(values['green_data']), \n",
    "                'red_ref': Phenology.get_reflectance(values['red_data']), \n",
    "                'NDVI': Phenology.calculate_NDVI(values['nir_data'], \n",
    "                                                 values['red_data'], \n",
    "                                                 values['longitudes'], \n",
    "                                                 values['latitudes'])}\n",
    "\n",
    "    def reproject(inpath, outpath, dst_crs='EPSG:4326'):\n",
    "        with rio.open(inpath) as src:\n",
    "            transform, width, height = calculate_default_transform(\n",
    "                src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "            kwargs = src.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': dst_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "            with rio.open(outpath, 'w', **kwargs) as dst:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rio.band(src, i),\n",
    "                        destination=rio.band(dst, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=dst_crs,\n",
    "                        resampling=Resampling.nearest)\n",
    "        return outpath\n",
    "                    \n",
    "    # returns cropped data and crops the tif file\n",
    "    def crop_raster_data(tif_path):\n",
    "        with rio.open(tif_path) as src:\n",
    "            extent_geojson = env['predecessor_outputs']['2']['extent_geojson']\n",
    "            out_image, out_transform = mask(src, shapes=[json.loads(extent_geojson)], crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                          \"height\": out_image.shape[1],\n",
    "                          \"width\": out_image.shape[2],\n",
    "                          \"transform\": out_transform})\n",
    "            # only needed for the lat lon coordinates\n",
    "            with rio.open(tif_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "            return out_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "def landsat_authentication(env, context):\n",
    "    return {'apikey' : Phenology.get_apikey(os.environ['USERNAME'], os.environ['PASSWORD'])}\n",
    "\n",
    "env['predecessor_outputs']['1'] = landsat_authentication(env, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "def landsat_get_scenes(env, context):\n",
    "    res = Phenology.get_scenes(env['predecessor_outputs']['1']['apikey'], \n",
    "                        os.environ['REGION'], env['static_input']['horizontal'], \n",
    "                        env['static_input']['vertical'],\n",
    "                        env['static_input']['start_date'], \n",
    "                        env['static_input']['end_date'])\n",
    "    scenes = []\n",
    "    for x in res['data']['results']:\n",
    "        scenes.append({'date': x['acquisitionDate'], 'entityId': x['entityId']})\n",
    "    with open(os.environ['GEOJSON']) as f:\n",
    "        geojson = json.load(f)\n",
    "    extent_geojson = geojson['features'][0]['geometry']\n",
    "    return {'response': scenes, 'count': len(scenes), 'extent_geojson': json.dumps(extent_geojson), 'apikey': env['predecessor_outputs']['1']['apikey']}\n",
    "                                    \n",
    "env['predecessor_outputs']['2'] = landsat_get_scenes(env, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "def landsat_get_stats(env, context):\n",
    "    entityId = env['predecessor_outputs']['2']['response']['entityId']\n",
    "    date = env['predecessor_outputs']['2']['response']['date']\n",
    "    tarname, url = Phenology.get_url(entityId, env['predecessor_outputs']['2']['apikey'])\n",
    "    Phenology.download(tarname, url)\n",
    "    stats = Phenology.get_statistical_values(tarname)\n",
    "    return {date : stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be parallelized\n",
    "stats = []\n",
    "res = env['predecessor_outputs']['2']\n",
    "for x in res['response']:\n",
    "    env['predecessor_outputs']['2']['response'] = x\n",
    "    stat = landsat_get_stats(env, context)\n",
    "    stats.append(stat)\n",
    "    # I just don't want this to take that long\n",
    "    if len(stats) == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "def landsat_determine_flowering(env, context):\n",
    "    stats = env['predecessor_outputs']['3']\n",
    "    results = {}\n",
    "    for date in stats:\n",
    "        results[date] = {'Flowering': True \n",
    "                        if not isinstance(stats[date]['NDVI']['avg'], str) and stats[date]['NDVI']['avg'] <= .425 and stats[date]['NDVI']['avg'] >= .375 \n",
    "                        else False, \n",
    "                        'Statistics': stats[date]}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should also be parallelized\n",
    "results = []\n",
    "for stat in stats:\n",
    "    env['predecessor_outputs']['3'] = stat\n",
    "    results.append(landsat_determine_flowering(env, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'2000-11-12': {'Flowering': False, 'Statistics': {'blue_ref': {'avg': 0.39, 'max': 0.76, 'min': 0.25, 'std': 0.12}, 'green_ref': {'avg': 0.4, 'max': 0.79, 'min': 0.24, 'std': 0.13}, 'red_ref': {'avg': 0.48, 'max': 0.85, 'min': 0.32, 'std': 0.12}, 'NDVI': {'avg': -0.86, 'max': -0.83, 'min': -0.9, 'std': 0.02, 'FF': 0.0, 'VF': 1.0, 'coordinates': '[[-121.7248, 46.7723]]'}}}}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
